# CrossVideoQA: A Benchmark for Cross-Video Reasoning

**CrossVideoQA** is a comprehensive benchmark dataset designed to evaluate **cross-video reasoning** capabilities, with a special focus on **human-centric queries** spanning different spatial locations and temporal periods.

---

## ğŸ§  Motivation

Cross-video understanding is essential for real-world applications such as:

- **Surveillance**: Tracking individuals across distributed camera networks (e.g., office buildings, transportation hubs).
- **Content Analysis**: Discovering related events from different perspectives to reconstruct coherent narratives.

Existing video QA datasets are limited to single-source settings. **CrossVideoQA** fills this gap by challenging models to reason across multiple videos in diverse spatial-temporal conditions.

---

## ğŸ“¦ Dataset Overview

CrossVideoQA integrates two complementary datasets:

- **Edinburgh Office Surveillance Dataset (EOSD)**  
  - 18 surveillance videos  
  - 3 indoor locations across 12 dates  
  - ~450,000 frames  
  - Suitable for structured, multi-day human behavior analysis  
  - Video length: 6â€“21 min (avg ~14 min)

- **HACS (Human Action Clips and Segments)**  
  - 50,000 web videos  
  - 1.55 million action clips  
  - High visual and semantic diversity  
  - Clip length: 9 sec â€“ 4 min (avg ~2 min)

**Combined Dataset:**  
> `DCrossVideoQA = DEOSD âˆª DHACS`

---

## ğŸ“Š Dataset Statistics & Structure

- **Total videos used:** 100  
- **Total QA pairs:** 197  
- **Average video duration:** ~3 min  
- **Average number of video segments per question:** 2.33  
- **Reasoning depth:** From low-level perception to high-level causal reasoning

### ğŸ§ª Task Categories

CrossVideoQA questions are categorized into:

| Category                      | Count |
|------------------------------|-------|
| Person Recognition           | 56    |
| Behavior Analysis            | 81    |
| Summarization & Causal Inference | 60    |

### ğŸ” Reasoning Modalities

Each QA pair falls under one of four spatial-temporal reasoning configurations:

| Modality               | Description                                              |
|------------------------|----------------------------------------------------------|
| `Msingle`              | Within a single video (same location, same day)          |
| `Mcross-spatial`       | Across multiple locations within the same time frame     |
| `Mcross-temporal`      | Same location but across different dates                 |
| `Mcross-spatiotemporal`| Full spatial-temporal integration                        |

This design enables systematic evaluation of models across increasing levels of complexity.

---

## ğŸ—ï¸ Benchmark Construction Pipeline

We adopt a three-stage QA construction process:

1. **Expert Annotation**  
   - Human-written exemplar QA pairs for each reasoning type and modality.

2. **LLM-Augmented Generation**  
   - Automatic generation under controlled prompts to enhance diversity.

3. **Human Validation**  
   - Manual filtering and correction to ensure:
     - Answerability
     - Accuracy
     - Balanced difficulty

---

## ğŸ“ Repository Structure

```plaintext
CrossVideoQA/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ EOSD/
â”‚   â””â”€â”€ HACS/
â”‚
â”œâ”€â”€ annotations/
â”‚   â”œâ”€â”€ qa_pairs.json           # All QA entries with metadata
â”‚   â””â”€â”€ splits/                 # Train/val/test partition info
â”‚
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ metrics.py              # Evaluation metrics for each task
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
